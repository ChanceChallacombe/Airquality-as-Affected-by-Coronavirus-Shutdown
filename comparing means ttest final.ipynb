{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CO-8hr\n",
      "OZONE-1HR\n",
      "OZONE-8HR\n",
      "PM2.5-24hr\n",
      "SO2-24HR\n"
     ]
    }
   ],
   "source": [
    "# all data files were downloaded from the airnow archive\n",
    "# Import the libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import pprint\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
    "                               AutoMinorLocator)\n",
    "import matplotlib.colors as mcolors\n",
    "import scipy.stats as stats\n",
    "import timeit\n",
    "legend=['CO-8hr in PPM','OZONE-1HR in PPB','OZONE-8HR in PPB','PM2.5-24hr in UG/M3','SO2-24HR in PPB']\n",
    "measure=['CO-8hr','OZONE-1HR','OZONE-8HR','PM2.5-24hr','SO2-24HR']\n",
    "def determine_city(data_path, measure):\n",
    "    unit=[]\n",
    "    measurement=[]\n",
    "    dictionary={}\n",
    "    city=[]\n",
    "    with open(data_path, encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            line = line.split('|')\n",
    "            if (line[3]==str(measure)):\n",
    "                city.append(line[2])\n",
    "                unit.append(line[4])\n",
    "                measurement.append(line[5])\n",
    "    for i in range (0,len(city)):\n",
    "        key=city[i]\n",
    "        val=measurement[i]\n",
    "        dictionary[key]=[float(val)]\n",
    "    return city\n",
    "\n",
    "\n",
    "\n",
    "CO8hr_2019=determine_city('/home/usr/airnowDat/2019/20190319-peak.dat', 'CO-8hr')\n",
    "OZONE1HR_2019=determine_city('/home/usr/airnowDat/2019/20190319-peak.dat', 'OZONE-1HR')\n",
    "OZONE8HR_2019=determine_city('/home/usr/airnowDat/2019/20190319-peak.dat', 'OZONE-8HR')\n",
    "PM1024hr_2019=determine_city('/home/usr/airnowDat/2019/20190319-peak.dat', 'PM10-24hr')\n",
    "PM2524hr_2019=determine_city('/home/usr/airnowDat/2019/20190319-peak.dat', 'PM2.5-24hr')\n",
    "SO224HR_2019=determine_city('/home/usr/airnowDat/2019/20190319-peak.dat', 'SO2-24HR')\n",
    "\n",
    "CO8hr_2020=determine_city('/home/usr/airnowDat/2020/20200319-peak.dat', 'CO-8hr')\n",
    "OZONE1HR_2020=determine_city('/home/usr/airnowDat/2020/20200319-peak.dat', 'OZONE-1HR')\n",
    "OZONE8HR_2020=determine_city('/home/usr/airnowDat/2020/20200319-peak.dat', 'OZONE-8HR')\n",
    "PM1024hr_2020=determine_city('/home/usr/airnowDat/2020/20200319-peak.dat', 'PM10-24hr')\n",
    "PM2524hr_2020=determine_city('/home/usr/airnowDat/2020/20200319-peak.dat', 'PM2.5-24hr')\n",
    "SO224HR_2020=determine_city('/home/usr/airnowDat/2020/20200319-peak.dat', 'SO2-24HR')\n",
    "\n",
    "def find_true_city_list(list1,list2):\n",
    "    set_true_list=set(list1).intersection(list2)\n",
    "    return(list(set_true_list))\n",
    "\n",
    "CO8hr=find_true_city_list(CO8hr_2019,CO8hr_2020)\n",
    "OZONE1HR=find_true_city_list(OZONE1HR_2019,OZONE1HR_2020)\n",
    "OZONE8HR=find_true_city_list(OZONE8HR_2019,OZONE8HR_2020)\n",
    "PM1024hr=find_true_city_list(PM1024hr_2019,PM1024hr_2020)\n",
    "PM2524hr=find_true_city_list(PM2524hr_2019,PM2524hr_2020)\n",
    "SO224HR=find_true_city_list(SO224HR_2019,SO224HR_2020)\n",
    "\n",
    "city_lists={}\n",
    "city_lists['CO-8hr']=CO8hr\n",
    "city_lists['OZONE-1HR']=OZONE1HR\n",
    "city_lists['OZONE-8HR']=OZONE8HR\n",
    "city_lists['PM10-24hr']=PM1024hr\n",
    "city_lists['PM2.5-24hr']=PM2524hr\n",
    "city_lists['SO2-24HR']=SO224HR\n",
    "\n",
    "n1={}\n",
    "\n",
    "n1['CO-8hr']=str(len(CO8hr_2019))\n",
    "n1['OZONE-1HR']=str(len(OZONE1HR_2019))\n",
    "n1['OZONE-8HR']=str(len(OZONE8HR_2019))\n",
    "n1['PM10-24hr']=str(len(PM1024hr_2019))\n",
    "n1['PM2.5-24hr']=str(len(PM2524hr_2019))\n",
    "n1['SO2-24HR']=str(len(SO224HR_2019))\n",
    "\n",
    "n2={}\n",
    "\n",
    "n2['CO-8hr']=str(len(CO8hr_2020))\n",
    "n2['OZONE-1HR']=str(len(OZONE1HR_2020))\n",
    "n2['OZONE-8HR']=str(len(OZONE8HR_2020))\n",
    "n2['PM10-24hr']=str(len(PM1024hr_2020))\n",
    "n2['PM2.5-24hr']=str(len(PM2524hr_2020))\n",
    "n2['SO2-24HR']=str(len(SO224HR_2020))\n",
    "\n",
    "n={}\n",
    "\n",
    "n['CO-8hr']=str(len(CO8hr))\n",
    "n['OZONE-1HR']=str(len(OZONE1HR))\n",
    "n['OZONE-8HR']=str(len(OZONE8HR))\n",
    "n['PM10-24hr']=str(len(PM1024hr))\n",
    "n['PM2.5-24hr']=str(len(PM2524hr))\n",
    "n['SO2-24HR']=str(len(SO224HR))\n",
    "\n",
    "kCO8hr=len(CO8hr)//10\n",
    "kOZONE1HR=len(OZONE1HR)//10\n",
    "kOZONE8HR=len(OZONE8HR)//10\n",
    "kPM1024hr=len(PM1024hr)//10\n",
    "kPM2524hr=len(PM2524hr)//10\n",
    "kSO224HR=len(SO224HR)//10\n",
    "\n",
    "k={}\n",
    "k['CO-8hr']=str(kCO8hr)\n",
    "k['OZONE-1HR']=str(kOZONE1HR)\n",
    "k['OZONE-8HR']=str(kOZONE8HR)\n",
    "k['PM10-24hr']=str(kPM1024hr)\n",
    "k['PM2.5-24hr']=str(kPM2524hr)\n",
    "k['SO2-24HR']=str(kSO224HR)\n",
    "\n",
    "\n",
    "sample_CO8hr_names=random.choices(CO8hr,k=kCO8hr)\n",
    "sample_OZONE1HR_names=random.choices(OZONE1HR,k=kOZONE1HR)\n",
    "sample_OZONE8HR_names=random.choices(OZONE8HR,k=kOZONE8HR)\n",
    "sample_PM1024hr_names=random.choices(PM1024hr,k=kPM1024hr)\n",
    "sample_PM2524hr_names=random.choices(PM2524hr,k=kPM2524hr)\n",
    "sample_SO224HR_names=random.choices(SO224HR,k=kSO224HR)\n",
    "\n",
    "samples={}\n",
    "samples['CO-8hr']=sample_CO8hr_names\n",
    "samples['OZONE-1HR']=sample_OZONE1HR_names\n",
    "samples['OZONE-8HR']=sample_OZONE8HR_names\n",
    "samples['PM10-24hr']=sample_PM1024hr_names\n",
    "samples['PM2.5-24hr']=sample_PM2524hr_names\n",
    "samples['SO2-24HR']=sample_SO224HR_names\n",
    "\n",
    "#pull out the specific measurement for each measure and location\n",
    "def add_means(data_path,location, measure):\n",
    "    \n",
    "    measurement=0\n",
    "    #open the data file from the data path parameter inputted in the function \n",
    "    with open(data_path, encoding='latin-1') as f:\n",
    "        #iterating through each line in the data file splitting by the delimiter '|'\n",
    "        for line in f:\n",
    "            line = line.split('|')\n",
    "            if (line[2]==str(location) and line[3]==str(measure)):\n",
    "                #add the respective data to the list that describes it\n",
    "                measurement=float(line[5])\n",
    "    \n",
    "    return measurement\n",
    "\n",
    "\n",
    "# create the mean data dictionaries\n",
    "def comparing_means_ttest(measure,city_list,sample_list,n,k):\n",
    "    measure_2018_data={}\n",
    "    for i in city_list:\n",
    "        key=i\n",
    "        data=[]\n",
    "        for i in range(19,32):\n",
    "            data.append(add_means(('/home/usr/airnowDat/2018/201803'+str(i)+'-peak.dat'),key,measure))\n",
    "        for i in range(1,10):\n",
    "            data.append(add_means(('/home/usr/airnowDat/2018/2018040'+str(i)+'-peak.dat'),key,measure))\n",
    "        for i in range(10,31):\n",
    "            data.append(add_means(('/home/usr/airnowDat/2018/201804'+str(i)+'-peak.dat'),key,measure))\n",
    "        measure_2018_data[key]=[round(np.mean(data),5),round(np.std(data),5)]\n",
    "    measure_2019_data={}\n",
    "    for i in city_list:\n",
    "        key=i\n",
    "        data=[]\n",
    "        for i in range(19,32):\n",
    "            data.append(add_means(('/home/usr/airnowDat/2019/201903'+str(i)+'-peak.dat'),key,measure))\n",
    "        for i in range(1,10):\n",
    "            data.append(add_means(('/home/usr/airnowDat/2019/2019040'+str(i)+'-peak.dat'),key,measure))\n",
    "        for i in range(10,31):\n",
    "            data.append(add_means(('/home/usr/airnowDat/2019/201904'+str(i)+'-peak.dat'),key,measure))\n",
    "        measure_2019_data[key]=[round(np.mean(data),5),round(np.std(data),5)]\n",
    "    measure_2020_data={}\n",
    "    for i in city_list:\n",
    "        key=i\n",
    "        data=[]\n",
    "        for i in range(19,32):\n",
    "            data.append(add_means(('/home/usr/airnowDat/2020/202003'+str(i)+'-peak.dat'),key,measure))\n",
    "        for i in range(1,10):\n",
    "            data.append(add_means(('/home/usr/airnowDat/2020/2020040'+str(i)+'-peak.dat'),key,measure))\n",
    "        for i in range(10,31):\n",
    "            data.append(add_means(('/home/usr/airnowDat/2020/202004'+str(i)+'-peak.dat'),key,measure))\n",
    "        measure_2020_data[key]=[round(np.mean(data),5),round(np.std(data),5)]\n",
    "    #create the difference in means/stdev dictionary\n",
    "    measure_2019sub2020={}\n",
    "    for i in city_list:\n",
    "        key=i\n",
    "        list1=[]\n",
    "        list1.append(round((measure_2019_data[key][0]-measure_2020_data[key][0]),3))\n",
    "        list1.append(round(math.sqrt((measure_2019_data[key][1]*measure_2019_data[key][1])+(measure_2020_data[key][1]*measure_2020_data[key][1])),3))\n",
    "        measure_2019sub2020[key]=list1\n",
    "        \n",
    "    sample_means=[]\n",
    "    ttest = open('ttest for '+str(measure)+'.txt', 'w') \n",
    "    for i in sample_list:\n",
    "        sample_means.append(measure_2019sub2020[i][0])\n",
    "    t,p=stats.ttest_1samp(sample_means,0)\n",
    "    Ho=('Ho: mu=0 \\nwhere mu is the true population mean of differences in 2019 and 2020(2019-2020) for '+\n",
    "        'all cities simmilar to those in the sample that measured:\\n'+\n",
    "        str(measure)+' for:\\n2019-2020\\n')\n",
    "    Ha=('Ha: mu>0 \\nwhere mu is the true population mean of differences in 2019 and 2020(2019-2020) for '+\n",
    "        'all cities simmilar to those in the sample that measured:\\n'+\n",
    "        str(measure)+' for:\\n2019-2020\\n')\n",
    "    ttest.write(Ho)\n",
    "    ttest.write(Ha)\n",
    "    ttest.write('n='+n+'\\na sample of size '+k+' was taken and '+k+'>30\\n')\n",
    "    ttest.write('The population size is at least '+str(len(city_list))+' and at least 10 times larger than '+k+'\\n')\n",
    "    ttest.write('The sample was randomly selected with random.choices\\nAll conditions for inference have been met\\n')\n",
    "    ttest.write('The t statistic is:\\n')\n",
    "    ttest.write(str(t)+'\\n')\n",
    "    ttest.write('The p value is:\\n')\n",
    "    ttest.write(str(p/2)+'\\n')\n",
    "    print(measure)\n",
    "    if ((p/2)>0.05):\n",
    "        ttest.write('Because the p  value is '+str(p/2)+' and greater than the significance level 0.05 '+ \n",
    "               'we fail to reject the null hypothesis.\\nThere is not convincing evidence that the '\n",
    "               'means for all cities simmilar to those in the sample that measured '\n",
    "                +str(measure)+' are lower in 2020 than in 2019')\n",
    "    else:\n",
    "        ttest.write('Because the p  value is '+str(p/2)+' and lower than the significance level 0.05 '+ \n",
    "               'we reject the null hypothesis.\\nThere is convincing evidence that the '\n",
    "               'means for all cities simmilar to those in the sample that measured '\n",
    "                +str(measure)+' are lower in 2020 than in 2019')\n",
    "\n",
    "\n",
    "\n",
    "for i in measure:\n",
    "    comparing_means_ttest(i,city_lists[i],samples[i],n[i],k[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
